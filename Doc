import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.util.List;
import java.util.stream.Collectors;
import java.util.stream.Stream;

public class WarehouseQueryBuilder {

    private static final Logger logger = LoggerFactory.getLogger(WarehouseQueryBuilder.class);
    private final BigQuerySqlQueryConfiguration bigQuerySqlQueryConfiguration;

    public WarehouseQueryBuilder(BigQuerySqlQueryConfiguration bigQuerySqlQueryConfiguration) {
        this.bigQuerySqlQueryConfiguration = bigQuerySqlQueryConfiguration;
    }

    public String generateSql(SqlServerPaginationRequest sqlRequest, String resultsTableName, String positionTableName, String productTableName, String cob) {
        String resultQueryTemplate = bigQuerySqlQueryConfiguration.getFtbQuery();

        List<ColumnVO> resultFields = transformColumnsByTablePrefix(sqlRequest.getValueCols(), "Result_");
        List<ColumnVO> positionFields = transformColumnsByTablePrefix(sqlRequest.getValueCols(), "Position_");
        List<ColumnVO> productFields = transformColumnsByTablePrefix(sqlRequest.getValueCols(), "Product_");

        addMandatoryFields(resultFields, positionFields, productFields);

        String pivotColumnValues = resultFields.stream()
                .map(ColumnVO::field)
                .map(value -> "'" + value + "'")
                .collect(Collectors.joining(", "));

        SqlQueryBuilder sqlQueryBuilder = new SqlQueryBuilder(sqlRequest);

        String selectValues = sqlQueryBuilder.selectSql(resultFields);
        String selectPositions = sqlQueryBuilder.selectSql(positionFields);
        String selectProducts = sqlQueryBuilder.selectSql(productFields);

        String finalQuery = buildFinalQuery(resultQueryTemplate, selectValues, selectPositions, selectProducts, pivotColumnValues, cob);

        logger.info("Generated SQL: {}", finalQuery);
        return finalQuery;
    }

    private List<ColumnVO> transformColumnsByTablePrefix(List<ColumnVO> columns, String prefix) {
        return columns.stream()
                .filter(column -> column.getField().startsWith(prefix))
                .map(column -> new ColumnVO(column.getField().substring(prefix.length()), column.getDisplayName(), column.getAggFunc()))
                .collect(Collectors.toList());
    }

    private void addMandatoryFields(List<ColumnVO>... fieldGroups) {
        // Add mandatory fields to respective column groups as needed
    }

    private String buildFinalQuery(String template, String selectValues, String selectPositions, String selectProducts, String pivotColumnValues, String cob) {
        // Use template.replace() to insert dynamic content into the SQL template
        return template;
    }

    // Additional helper methods as needed...
}
import org.springframework.boot.web.reactive.error.ErrorWebExceptionHandler;
import org.springframework.core.annotation.Order;
import org.springframework.core.io.buffer.DataBufferFactory;
import org.springframework.http.HttpStatus;
import org.springframework.http.server.reactive.ServerHttpResponse;
import org.springframework.stereotype.Component;
import org.springframework.web.server.ServerWebExchange;
import reactor.core.publisher.Mono;

@Component
@Order(-2) // To ensure this handler runs before the default error handler
public class GlobalErrorWebExceptionHandler implements ErrorWebExceptionHandler {

    @Override
    public Mono<Void> handle(ServerWebExchange exchange, Throwable ex) {
        ServerHttpResponse response = exchange.getResponse();
        HttpStatus status = HttpStatus.INTERNAL_SERVER_ERROR; // Default to 500, but you can customize this based on the exception
        String message = "An unexpected error occurred"; // Default message

        // Customize the response status and message based on specific exceptions
        if (ex instanceof SpecificException) {
            status = HttpStatus.BAD_REQUEST; // For example, set to BAD_REQUEST for a specific type of exception
            message = "Specific error message for the user";
        }

        // Set the response status
        response.setStatusCode(status);

        // Write the custom error message to the response
        DataBufferFactory bufferFactory = response.bufferFactory();
        return response.writeWith(Mono.just(bufferFactory.wrap(message.getBytes())));
    }
}

WITH SourceData AS (
  SELECT
    PRICE,
    DELTA,
    RHO,
    EPI,
    RiskFactor
  FROM
    RiskEngine.collation_FTB_adhoc,
    UNNEST(Exposure) AS Exp
),
PositionData AS (
  SELECT
    ID AS Position_Id,
    InstrumentId AS Position_InstrumentId
  FROM
    warehouse_refdata_ppe.Positions-HBAP-2024-02-08
),
ProductData AS (
  SELECT
    ID AS Product_Id
  FROM
    warehouse_refdata_ppe.Products-HBAP-2024-02-08
),
JoinedData AS (
  SELECT
    s.*,
    p.Position_Id,
    pd.Product_Id
  FROM
    SourceData s
  JOIN
    PositionData p
  ON
    s.EPI = p.Position_Id
  JOIN
    ProductData pd
  ON
    p.Position_InstrumentId = pd.Product_Id
)
SELECT
  Result_RiskFactor,
  PRICE,
  DELTA,
  RHO
FROM
  JoinedData
PIVOT (
  SUM(Value) FOR Measure IN ('PRICE', 'DELTA', 'RHO')
)

public enum QueryValidationRules {
    VALIDATE_RISK_MEASURE_TYPE((context, dataDictionaryManager) -> {
        // Example: validate based on RiskMeasureType
        return dataDictionaryManager.validateRiskMeasureSpecifications(
            context.getRiskMeasureType(), context.getUnderliersCount(), context.getLabels());
    }),
    // Additional rules can be defined here

    private final ValidationRule rule;

    QueryValidationRules(ValidationRule rule) {
        this.rule = rule;
    }

    public ValidationResult validate(QueryContext context, DataDictionaryManager dataDictionaryManager) {
        return rule.apply(context, dataDictionaryManager);
    }
}



import java.util.Arrays;

public class QueryValidator {
    private final DataDictionaryManager dataDictionaryManager;
    private final List<QueryValidationRules> validationRules = Arrays.asList(
        QueryValidationRules.VALIDATE_RISK_MEASURE_TYPE
        // List other rules as necessary
    );

    public QueryValidator(DataDictionaryManager dataDictionaryManager) {
        this.dataDictionaryManager = dataDictionaryManager;
    }

    public ValidationResult validate(QueryContext context) {
        for (QueryValidationRules rule : validationRules) {
            ValidationResult result = rule.validate(context, dataDictionaryManager);
            if (!result.isValid()) {
                return result; // Return the first encountered validation failure with detailed feedback
            }
        }
        return ValidationResult.valid(); // All validations passed
    }
}

import java.util.Map;
import java.util.Optional;
import java.util.concurrent.ConcurrentHashMap;

public class DataDictionaryManager {
    private final Map<RiskMeasureType, RiskMeasureSpecs> riskMeasureSpecsMap = new ConcurrentHashMap<>();

    public DataDictionaryManager() {
        initialize();
    }

    private void initialize() {
        // Initialize with example specifications, indicating the allowed number of underliers and labels
        riskMeasureSpecsMap.put(RiskMeasureType.POINT, new RiskMeasureSpecs(RiskMeasureType.POINT, 2, 0));
        riskMeasureSpecsMap.put(RiskMeasureType.VECTOR, new RiskMeasureSpecs(RiskMeasureType.VECTOR, 1, 1));
        riskMeasureSpecsMap.put(RiskMeasureType.MATRIX, new RiskMeasureSpecs(RiskMeasureType.MATRIX, 1, 2));
    }

    public Optional<RiskMeasureSpecs> getRiskMeasureSpecs(RiskMeasureType type) {
        return Optional.ofNullable(riskMeasureSpecsMap.get(type));
    }
    
    // Method to provide detailed validation feedback
    public ValidationResult validateRiskMeasureSpecifications(RiskMeasureType type, int underliersCount, List<String> labels) {
        return getRiskMeasureSpecs(type).map(specs -> {
            if (underliersCount > specs.getMaxUnderliers()) {
                return ValidationResult.invalid("Exceeds maximum underliers for " + type);
            }
            if (labels.size() > specs.getMaxLabels()) {
                return ValidationResult.invalid("Exceeds maximum labels for " + type);
            }
            return ValidationResult.valid();
        }).orElse(ValidationResult.invalid("Risk measure type " + type + " not found."));
    }
}


public class QueryValidator {
    private final DataDictionaryManager dataDictionaryManager;

    public QueryValidator(DataDictionaryManager dataDictionaryManager) {
        this.dataDictionaryManager = dataDictionaryManager;
    }

    public ValidationResult validate(QueryContext context) {
        // Validate RiskMeasureType and its specifications
        ValidationResult typeSpecValidation = dataDictionaryManager.validateRiskMeasureSpecifications(
                context.getRiskMeasureType(), context.getUnderliersCount(), context.getLabels());
        if (!typeSpecValidation.isValid()) {
            return typeSpecValidation; // Return detailed feedback from specification validation
        }

        // Additional validations can be performed here as needed...
        
        return ValidationResult.valid(); // Default to valid if all checks pass
    }
}

public class QueryContext {
    private final RiskMeasureType riskMeasureType;
    private final int underliersCount;
    private final List<String> labels;

    public QueryContext(RiskMeasureType riskMeasureType, int underliersCount, List<String> labels) {
        this.riskMeasureType = riskMeasureType;
        this.underliersCount = underliersCount;
        this.labels = labels;
    }

    // Getters
    public RiskMeasureType getRiskMeasureType() {
        return riskMeasureType;
    }

    public int getUnderliersCount() {
        return underliersCount;
    }

    public List<String> getLabels() {
        return labels;
    }
}


public enum QueryValidationRules implements QueryValidationRule {
    VALIDATE_RISK_MEASURE_TYPE(context -> {
        // Example validation logic
        if (context.getRiskMeasureType() == null) {
            return ValidationResult.invalid("Risk measure type is missing.");
        }
        return ValidationResult.valid();
    }),
    // Other rules...

    private final QueryValidationRule rule;

    QueryValidationRules(QueryValidationRule rule) {
        this.rule = rule;
    }

    @Override
    public ValidationResult validate(QueryContext context) {
        return rule.validate(context);
    }
}
public interface QueryValidationRule {
    ValidationResult validate(QueryContext context);
}

public class ValidationResult {
    private final boolean valid;
    private final String message;

    private ValidationResult(boolean valid, String message) {
        this.valid = valid;
        this.message = message;
    }

    public static ValidationResult valid() {
        return new ValidationResult(true, "");
    }

    public static ValidationResult invalid(String message) {
        return new ValidationResult(false, message);
    }

    // Getters
    public boolean isValid() {
        return valid;
    }

    public String getMessage() {
        return message;
    }
}


public class RiskMeasureSpecs {
    private final RiskMeasureType type;
    private final int maxUnderliers;
    private final int maxLabels;

    public RiskMeasureSpecs(RiskMeasureType type, int maxUnderliers, int maxLabels) {
        this.type = type;
        this.maxUnderliers = maxUnderliers;
        this.maxLabels = maxLabels;
    }

    // Getters
    public RiskMeasureType getType() {
        return type;
    }

    public int getMaxUnderliers() {
        return maxUnderliers;
    }

    public int getMaxLabels() {
        return maxLabels;
    }
}

# Integration Document for New Schema Adoption and Constraint Management

## 1. Introduction

This document outlines the design and implementation strategy for adopting a new schema in our Java Spring WebFlux API, specifically aimed at dynamic request modeling for BigQuery SQL queries. The new schema introduces `Measure` and `Exposure` columns, categorizing data into point, vector, and matrix risks, each with its own set of rules and constraints. Our objective is to seamlessly integrate these changes, ensuring our system remains robust, flexible, and efficient.

## 2. Schema Update and Data Model Adaptation

### 2.1 Objective

Incorporate the new schema changes into the existing SQL builder logic, focusing on the integration of `Measure` and `Exposure` columns and the distinct handling required for point, vector, and matrix data types.

### 2.2 Design Considerations

- **Dynamic Schema Recognition**: Enhance the SQL builder to dynamically recognize and process requests involving the new `Measure` and `Exposure` columns.
- **Data Shape Handling**: Implement logic to identify the data shape (point, vector, matrix) based on `Measure` and `Exposure` attributes in the request model.
- **Rules and Constraints**: Codify rules for handling empty or required fields in `RiskFactor1`, `RiskFactor2`, `Label1`, and `Label2`, ensuring compliance with the new schema's requirements.

## 3. Metadata Management and Configuration

### 3.1 Objective

Develop a comprehensive data dictionary that details risk measures, including their dimensionality and expected number of underliers/risk factors, and make it accessible via an API endpoint.

### 3.2 Design Considerations

- **Dynamic Data Dictionary**: Create a central data dictionary that can be dynamically updated and fetched, serving as a crucial metadata source for the API.
- **API Endpoint for Discovery**: Expose the data dictionary through a dedicated API endpoint, allowing clients and developers to discover and understand available data fields, measures, and constraints.

## 4. Request Validation and SQL Query Building

### 4.1 Objective

Enhance the API's request validation and SQL query building processes to accommodate the new schema's rules and constraints, particularly regarding the mixing of risk types and the handling of dynamic measures lists.

### 4.2 Design Considerations

- **Request Constraints Enforcement**: Implement logic to validate request models against the new schema constraints, rejecting invalid combinations of risk types and ensuring compliance with `Measure` and `Exposure` requirements.
- **Enhanced SQL Builder Logic**: Update the SQL builder to support dynamic measures lists, incorporating logic for predefined measure lists and considering future enhancements for wildcard measure support.

## 5. BigQuery Integration and Handling

### 5.1 Objective

Address specific integration challenges with BigQuery, including identifier constraints and the need for pivoting operations to support exposure-related aggregations.

### 5.2 Design Considerations

- **Identifier Mapping**: Implement a mapping mechanism to transform problematic identifiers into BigQuery-compliant names.
- **Pivoting Support**: Ensure the SQL builder is capable of performing necessary pivoting operations, facilitating effective exposure-related data aggregation.

## 6. API Filtering, Aggregation, and Metadata Access

### 6.1 Objective

Refine the API's filtering logic and enhance the mechanisms for accessing job metadata, improving the system's ability to process and respond to complex queries.

### 6.2 Design Considerations

- **Filtering Logic Standardization**: Establish a clear and standardized approach for API filtering, clearly distinguishing between different types of filters.
- **Job Metadata Endpoints**: Develop endpoints to provide access to job metadata, supporting detailed queries and data retrieval based on specific job characteristics.

## 7. Documentation and Testing

### 7.1 Objective

Update all relevant documentation to accurately reflect the new features and changes, and conduct comprehensive testing to ensure the updated API meets all functional requirements.

### 7.2 Design Considerations

- **Documentation Update**: Thoroughly revise the API documentation, including detailed descriptions of the new schema, data dictionary access, and updated request validation rules.
- **Comprehensive Testing**: Perform extensive testing across various scenarios to validate the functionality of the updated API, focusing on schema integration, data handling, and compliance with defined constraints.

## 8. Conclusion

Adopting the new schema and managing the associated constraints require a holistic approach, encompassing schema integration, metadata management, request validation, BigQuery handling, and enhanced documentation and testing. By following the outlined design considerations and implementation strategies, we can ensure a seamless transition to the new schema, maintaining the robustness and efficiency of our system while accommodating the complex requirements of dynamic financial data analysis.

This document serves as a blueprint for the development team, guiding the integration process and ensuring all technical and functional requirements are met with precision and clarity.



Given the detailed API and implementation constraints you've provided, translating these into well-designed requirements for development and documentation purposes involves structuring them to address each specific constraint systematically. These can be formatted into JIRA tasks or documentation sections to guide the development process.

### Schema and Data Model Adaptation

**Task 1: Adapt API to Handle New Schema Constraints**
- **Summary**: Update the API to incorporate new schema changes, including handling for Measures and Exposure columns with specific rules for RiskFactors and Labels.
- **Description**: Implement logic to ensure:
  - Exposure RiskFactor1 can be optional; mandatory for certain measures like THETA.
  - RiskFactor2 can be optional; if present, RiskFactor1 must also be present, especially for measures like CROSS_GAMMA.
  - Label1 is always required; Label2 can be optional unless for specific measures like VEGA BUCKETS where both are needed.
- **Acceptance Criteria**: API correctly processes requests based on new schema rules, rejecting invalid requests with appropriate error messages.

### Metadata and Configuration Management

**Task 2: Develop a Dynamic Data Dictionary**
- **Summary**: Create a data dictionary that provides metadata about risk measures, including dimensionality and expected number of underliers/risk factors.
- **Description**: The dictionary should be accessible via an API endpoint for discovery purposes. It could initially use hardcoded values but should ideally fetch data from a centralized configuration source.
- **Acceptance Criteria**: API endpoint provides accurate metadata for each risk measure, facilitating dynamic request validation and SQL query building.

### Request Validation and SQL Query Construction

**Task 3: Implement Request Constraints and SQL Builder Logic**
- **Summary**: Enhance the SQL builder to respect constraints on mixing risk types and to support predefined lists of measures for query construction.
- **Description**: 
  - Enforce API-level constraints to prevent invalid combinations of point/vector/matrix risks.
  - Modify SQL builder to use an up-front list of measures for query construction, considering potential future support for wildcard measures through pre-queries.
- **Acceptance Criteria**: API rejects invalid risk combinations and dynamically constructs SQL queries based on valid request models and measure lists.

### BigQuery Integration and Pivoting

**Task 4: Address BigQuery Identifier Constraints and Pivoting Requirements**
- **Summary**: Update API and SQL builder to manage BigQuery identifier constraints and support necessary pivoting operations for aggregation.
- **Description**: 
  - Implement logic to handle BigQuery's identifier constraints, possibly mapping problematic identifiers to compliant ones.
  - Ensure the API and SQL builder can perform pivoting operations as required, especially for exposure-related aggregations.
- **Acceptance Criteria**: API and SQL builder accommodate BigQuery's limitations, successfully performing pivoting and aggregation tasks.

### API Filtering, Aggregation, and Job Metadata Handling

**Task 5: Standardize API Filtering and Job Metadata Access**
- **Summary**: Refine API filtering logic and enhance support for accessing job metadata.
- **Description**: 
  - Clarify and implement the correct layout for API filtering, distinguishing between snap, jobID, COB, and other filters.
  - Provide endpoints for accessing job metadata to facilitate queries related to specific runs or calculations.
- **Acceptance Criteria**: API users can effectively filter data and access job-related metadata, improving the ability to retrieve relevant risk calculations.

### Documentation and Testing

**Task 6: Documentation Update and Comprehensive Testing**
- **Summary**: Update documentation to reflect new features and changes, and conduct comprehensive testing of the updated API.
- **Description**: 
  - Revise API documentation to include information on new schema changes, data dictionary access, request validation rules, and filtering capabilities.
  - Perform thorough testing to ensure all new features and constraints are correctly implemented and function as expected.
- **Acceptance Criteria**: Updated documentation accurately reflects API functionality, and testing confirms the API behaves as intended across various scenarios.

By breaking down these overarching constraints and requirements into manageable tasks, you can systematically address each aspect of the API and schema update, ensuring a well-documented, robust implementation that meets the needs of dynamic request processing and complex data handling in BigQuery.

Schema and Data Model AdaptationTask 1: Adapt API to Handle New Schema ConstraintsSummary: Update the API to incorporate new schema changes, including handling for Measures and Exposure columns with specific rules for RiskFactors and Labels.Description: Implement logic to ensure:Exposure RiskFactor1 can be optional; mandatory for certain measures like THETA.RiskFactor2 can be optional; if present, RiskFactor1 must also be present, especially for measures like CROSS_GAMMA.Label1 is always required; Label2 can be optional unless for specific measures like VEGA BUCKETS where both are needed.Acceptance Criteria: API correctly processes requests based on new schema rules, rejecting invalid requests with appropriate error messages.Metadata and Configuration ManagementTask 2: Develop a Dynamic Data DictionarySummary: Create a data dictionary that provides metadata about risk measures, including dimensionality and expected number of underliers/risk factors.Description: The dictionary should be accessible via an API endpoint for discovery purposes. It could initially use hardcoded values but should ideally fetch data from a centralized configuration source.Acceptance Criteria: API endpoint provides accurate metadata for each risk measure, facilitating dynamic request validation and SQL query building.Request Validation and SQL Query ConstructionTask 3: Implement Request Constraints and SQL Builder LogicSummary: Enhance the SQL builder to respect constraints on mixing risk types and to support predefined lists of measures for query construction.Description:Enforce API-level constraints to prevent invalid combinations of point/vector/matrix risks.Modify SQL builder to use an up-front list of measures for query construction, considering potential future support for wildcard measures through pre-queries.Acceptance Criteria: API rejects invalid risk combinations and dynamically constructs SQL queries based on valid request models and measure lists.BigQuery Integration and PivotingTask 4: Address BigQuery Identifier Constraints and Pivoting RequirementsSummary: Update API and SQL builder to manage BigQuery identifier constraints and support necessary pivoting operations for aggregation.Description:Implement logic to handle BigQuery's identifier constraints, possibly mapping problematic identifiers to compliant ones.Ensure the API and SQL builder can perform pivoting operations as required, especially for exposure-related aggregations.Acceptance Criteria: API and SQL builder accommodate BigQuery's limitations, successfully performing pivoting and aggregation tasks.API Filtering, Aggregation, and Job Metadata HandlingTask 5: Standardize API Filtering and Job Metadata AccessSummary: Refine API filtering logic and enhance support for accessing job metadata.Description:Clarify and implement the correct layout for API filtering, distinguishing between snap, jobID, COB, and other filters.Provide endpoints for accessing job metadata to facilitate queries related to specific runs or calculations.Acceptance Criteria: API users can effectively filter data and access job-related metadata, improving the ability to retrieve relevant risk calculations.Documentation and TestingTask 6: Documentation Update and Comprehensive TestingSummary: Update documentation to reflect new features and changes, and conduct comprehensive testing of the updated API.Description:Revise API documentation to include information on new schema changes, data dictionary access, request validation rules, and filtering capabilities.Perform thorough testing to ensure all new features and constraints are correctly implemented and function as expected.Acceptance Criteria: Updated documentation accurately reflects API functionality, and testing confirms the API behaves as intended across various scenarios.

import org.springframework.core.io.buffer.DataBuffer;
import org.springframework.core.io.buffer.DataBufferUtils;
import org.springframework.http.server.reactive.ServerHttpRequestDecorator;
import org.springframework.web.server.ServerWebExchange;
import org.springframework.web.server.WebFilter;
import org.springframework.web.server.WebFilterChain;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

import java.nio.charset.Charset;

public class BodyCachingWebFilter implements WebFilter {

    @Override
    public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
        return DataBufferUtils.join(exchange.getRequest().getBody())
                .flatMap(dataBuffer -> {
                    byte[] bytes = new byte[dataBuffer.readableByteCount()];
                    dataBuffer.read(bytes);
                    DataBufferUtils.release(dataBuffer);

                    String body = new String(bytes, Charset.forName("UTF-8"));
                    exchange.getAttributes().put("cachedRequestBody", body); // Store body as an attribute

                    // Reconstruct the request with cached body
                    ServerHttpRequestDecorator decoratedRequest = new ServerHttpRequestDecorator(exchange.getRequest()) {
                        @Override
                        public Flux<DataBuffer> getBody() {
                            return Flux.just(exchange.getResponse().bufferFactory().wrap(bytes));
                        }
                    };

                    return chain.filter(exchange.mutate().request(decoratedRequest).build());
                });
    }
}


@Override
public Mono<Void> writeWith(Publisher<? extends DataBuffer> body) {
    Mono<DataBuffer> modifiedBody = Mono.from(body)
        .flatMap(dataBuffer -> {
            try {
                log.info("Reading response body");
                String responseBody = StandardCharsets.UTF_8.decode(dataBuffer.asByteBuffer()).toString();

                log.info("Modifying response body");
                String modifiedResponse = modifyResponseBody(responseBody, exchange.getRequest().getHeaders().get(REQUEST_HEADER_ORIGINAL_HOST));

                byte[] bytes = modifiedResponse.getBytes(StandardCharsets.UTF_8);
                DataBufferUtils.release(dataBuffer);
                
                log.info("Creating new data buffer");
                return Mono.just(originalResponse.bufferFactory().wrap(bytes));
            } catch (Exception e) {
                log.error("Error during response modification", e);
                return Mono.error(e);
            }
        });

    return super.writeWith(modifiedBody)
        .doOnTerminate(() -> log.info("Response writing terminated"))
        .doOnError(error -> log.error("Error writing response", error));
}

To incorporate logging at each step and provide the complete `WebFilter` component, you can use `log.info` statements. Here's the complete revised code with logging:

```java
import org.springframework.core.io.buffer.DataBuffer;
import org.springframework.core.io.buffer.DataBufferUtils;
import org.springframework.http.server.reactive.ServerHttpResponse;
import org.springframework.http.server.reactive.ServerHttpResponseDecorator;
import org.springframework.web.server.ServerWebExchange;
import org.springframework.web.server.WebFilter;
import org.springframework.web.server.WebFilterChain;
import reactor.core.publisher.Mono;
import reactor.core.publisher.Flux;
import org.reactivestreams.Publisher;

import java.nio.charset.StandardCharsets;
import java.util.List;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MyWebFilter implements WebFilter {

    private static final Logger log = LoggerFactory.getLogger(MyWebFilter.class);
    private static final String SWAGGER_CONFIG_API_PATH = "/your-swagger-path";
    private static final String REQUEST_HEADER_ORIGINAL_HOST = "your-header";
    private static final String DYNAMIC_PLACEHOLDER = "your-placeholder";

    @Override
    public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
        log.info("Filtering request: {}", exchange.getRequest().getURI());
        if (exchange.getRequest().getURI().getPath().contains(SWAGGER_CONFIG_API_PATH)) {
            ServerHttpResponse originalResponse = exchange.getResponse();
            ServerHttpResponseDecorator decoratedResponse = new ServerHttpResponseDecorator(originalResponse) {
                @Override
                public Mono<Void> writeWith(Publisher<? extends DataBuffer> body) {
                    Mono<DataBuffer> modifiedBody = Mono.from(body)
                        .map(dataBuffer -> {
                            try {
                                log.info("Reading response body");
                                String responseBody = StandardCharsets.UTF_8.decode(dataBuffer.asByteBuffer()).toString();

                                log.info("Modifying response body");
                                String modifiedResponse = modifyResponseBody(responseBody, exchange.getRequest().getHeaders().get(REQUEST_HEADER_ORIGINAL_HOST));

                                byte[] bytes = modifiedResponse.getBytes(StandardCharsets.UTF_8);
                                return originalResponse.bufferFactory().wrap(bytes);
                            } finally {
                                log.info("Releasing data buffer");
                                DataBufferUtils.release(dataBuffer);
                            }
                        });

                    log.info("Writing modified response body");
                    return super.writeWith(modifiedBody);
                }
            };

            log.info("Continuing filter chain with decorated response");
            return chain.filter(exchange.mutate().response(decoratedResponse).build());
        }

        log.info("URI path does not contain Swagger config path, continuing filter chain normally");
        return chain.filter(exchange);
    }

    private String modifyResponseBody(String responseBody, List<String> originalHosts) {
        log.info("Modifying response body based on original hosts");
        if (originalHosts != null && !originalHosts.isEmpty()) {
            return responseBody.replace(DYNAMIC_PLACEHOLDER, originalHosts.get(0));
        }
        return responseBody;
    }
}
```

In this updated version, `log.info` statements are added at each major step to help with debugging and tracking the flow of the request and response. Make sure you have SLF4J and a logging implementation (like Logback) configured in your project to see these logs. Adjust the constants like `

pipeline {
    agent any

    stages {
        stage('Get Artifact Name') {
            steps {
                script {
                    // Construct the URL for metadata.xml
                    def metadataUrl = "${nexusRepoURL}/${groupId.replace('.', '/')}/${artifactId}/${version}/maven-metadata.xml"

                    // Download the metadata.xml file
                    sh "curl -o metadata.xml ${metadataUrl}"

                    // Parse the metadata.xml to get the latest snapshot version
                    def metadataXml = readFile('metadata.xml')
                    def matcher = metadataXml =~ /<value>(.+)<\/value>/
                    def latestSnapshotVersion = ''
                    if (matcher.matches()) {
                        latestSnapshotVersion = matcher.group(1)
                    }

                    // Construct the artifact filename
                    def artifactFileName = "${artifactId}-${latestSnapshotVersion}.jar" // Change the extension if needed

                    // Print the artifact name
                    echo "Latest artifact name: ${artifactFileName}"
                }
            }
        }
    }
}

@Override
    public Mono<Void> filter(ServerWebExchange exchange, WebFilterChain chain) {
        if (exchange.getRequest().getURI().getPath().contains(SWAGGER_CONFIG_API_PATH)) {
            ServerHttpResponse originalResponse = exchange.getResponse();
            ServerHttpResponseDecorator decoratedResponse = new ServerHttpResponseDecorator(originalResponse) {
                @Override
                public Mono<Void> writeWith(Flux<? extends DataBuffer> body) {
                    Flux<DataBuffer> modifiedBody = body.map(dataBuffer -> {
                        // Read the dataBuffer into a String
                        String responseBody = StandardCharsets.UTF_8.decode(dataBuffer.asByteBuffer()).toString();
                        // Release the dataBuffer to prevent memory leaks
                        DataBufferUtils.release(dataBuffer);
                        
                        // Modify the responseBody as needed
                        String modifiedResponse = modifyResponseBody(responseBody, exchange.getRequest().getHeaders().get(REQUEST_HEADER_ORIGINAL_HOST));
                        
                        // Convert the modified response back into a DataBuffer
                        byte[] bytes = modifiedResponse.getBytes(StandardCharsets.UTF_8);
                        return originalResponse.bufferFactory().wrap(bytes);
                    });

                    // Write the modified body
                    return super.writeWith(modifiedBody);
                }
            };

            // Continue the filter chain with the decorated response
            return chain.filter(exchange.mutate().response(decoratedResponse).build());
        }

        // If the URI path does not contain the Swagger config path, continue the filter chain normally
        return chain.filter(exchange);
    }

    private String modifyResponseBody(String responseBody, List<String> originalHosts) {
        // Replace the dynamic placeholder with the original host from the request headers
        if (originalHosts != null && !originalHosts.isEmpty()) {
            return responseBody.replace(DYNAMIC_PLACEHOLDER, originalHosts.get(0));
        }
        return responseBody;
    }
}


