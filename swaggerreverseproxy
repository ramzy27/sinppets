Deimport org.springdoc.core.SwaggerUiConfigParameters;
import org.springdoc.core.SwaggerUiOAuthProperties;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Configuration;

@Configuration
public class SwaggerAggregatorConfig {

    @Value("${cloud.service.api-docs-url}")
    private String cloudServiceApiDocsUrl;

    public SwaggerAggregatorConfig(SwaggerUiConfigParameters swaggerUiConfigParameters, SwaggerUiOAuthProperties swaggerUiOAuthProperties) {
        swaggerUiConfigParameters.addGroup("Cloud Service")
                .url(cloudServiceApiDocsUrl);
    }
}
cloud.service.api-docs-url=http://gcp-cloud-service-url/v3/api-docs
spring:
  cloud:
    gateway:
      routes:
        - id: gcp-cloud-service
          uri: http://gcp-cloud-service-url
          predicates:
            - Path=/gcp-cloud-service/**



<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.example</groupId>
    <artifactId>mongo-to-bigquery</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <packaging>jar</packaging>

    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.6.3</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>

    <properties>
        <java.version>11</java.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-mongodb</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.cloud</groupId>
            <artifactId>spring-cloud-gcp-starter-storage</artifactId>
            <version>2.0.5</version>
        </dependency>
        <dependency>
            <groupId>com.google.cloud</groupId>
            <artifactId>spring-cloud-gcp-starter-bigquery</artifactId>
            <version>2.0.5</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>



import com.google.cloud.bigquery.*;
import com.google.cloud.storage.BlobInfo;
import com.google.cloud.storage.Storage;
import com.google.gson.Gson;
import org.bson.Document;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.nio.channels.Channels;
import java.nio.charset.StandardCharsets;
import java.util.List;

@Service
public class MongoToBigQueryTransferService {

    private static final Logger logger = LoggerFactory.getLogger(MongoToBigQueryTransferService.class);

    @Autowired
    private MongoTemplate mongoTemplate;

    @Autowired
    private Storage storage;

    @Autowired
    private BigQuery bigQuery;

    public void transferData(String collectionName, String bucketName, String gcsObjectName, String datasetName, String tableName) {
        // Export MongoDB data to JSON
        List<Document> documents = mongoTemplate.findAll(Document.class, collectionName);
        String jsonContent = new Gson().toJson(documents);

        // Upload JSON data to Google Cloud Storage
        BlobInfo blobInfo = BlobInfo.newBuilder(BlobId.of(bucketName, gcsObjectName)).build();
        storage.createFrom(Channels.newInputStream(Channels.newChannel(jsonContent.getBytes(StandardCharsets.UTF_8).newInputStream())), blobInfo);

        // Load data from GCS to BigQuery
        TableId tableId = TableId.of(datasetName, tableName);
        LoadJobConfiguration loadJobConfiguration = LoadJobConfiguration.newBuilder(tableId, "gs://" + bucketName + "/" + gcsObjectName)
                .setFormatOptions(FormatOptions.json())
                .setAutodetect(true)
                .build();

        Job loadJob = bigQuery.create(JobInfo.of(loadJobConfiguration));
        try {
            Job completedJob = loadJob.waitFor();
            if (completedJob.getStatus().getError() != null) {
                logger.error("Error loading data to BigQuery: {}", completedJob.getStatus().getError().toString());
            } else {
                logger.info("Data loaded successfully to BigQuery");
            }
        } catch (InterruptedException e) {
            logger.error("Error waiting for load job to complete", e);
            Thread.currentThread().interrupt();
        }
    }
}

import org.springdoc.core.GroupedOpenApi;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class SwaggerAggregatorConfig {

    @Value("${onpremise.server.url}")
    private String onPremiseServerUrl;

    @Bean
    public GroupedOpenApi cloudServiceOpenApi() {
        String cloudServiceApiDocsUrl = onPremiseServerUrl + "/cloud-service-api-docs";
        return GroupedOpenApi.builder()
                .group("Cloud Service")
                .pathsToMatch("/**")
                .openApiCustomiser(api -> api.servers(List.of(new Server().url(cloudServiceApiDocsUrl))))
                .build();
    }
}



import org.springframework.beans.factory.annotation.Value;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.client.RestTemplate;

@RestController
public class CloudServiceApiDocsController {

    @Value("${cloud.service.api-docs-url}")
    private String cloudServiceApiDocsUrl;

    @GetMapping("/cloud-service-api-docs")
    public ResponseEntity<String> fetchCloudServiceApiDocs() {
        RestTemplate restTemplate = new RestTemplate();
        return restTemplate.getForEntity(cloudServiceApiDocsUrl, String.class);
    }
}

import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.InsertAllRequest;
import com.google.cloud.bigquery.InsertAllResponse;
import com.google.cloud.bigquery.TableId;
import com.mongodb.client.MongoCollection;
import org.bson.Document;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;

@Service
public class MongoToBigQueryTransferService {

    @Autowired
    private MongoDbService mongoDbService;

    @Autowired
    private BigQueryService bigQueryService;

    public void transferData() {
        MongoCollection<Document> collection = mongoDbService.getCollection();
        TableId tableId = bigQueryService.getTableId();
        BigQuery bigquery = bigQueryService.getBigQueryInstance();

        // Ensure table exists with the correct schema and partitioning
        bigQueryService.createTableIfNotExists(collection, tableId);

        // Transfer data
        List<InsertAllRequest.RowToInsert> rows = mongoDbService.getRowsFromCollection(collection);
        InsertAllRequest insertRequest = InsertAllRequest.newBuilder(tableId).setRows(rows).build();
        InsertAllResponse insertResponse = bigquery.insertAll(insertRequest);

        if (insertResponse.hasErrors()) {
            System.err.println("Errors occurred while inserting rows:");
            insertResponse.getInsertErrors().forEach((index, errors) -> {
                System.err.println("Row index: " + index);
                errors.forEach(error -> System.err.println("Error: " + error.getMessage()));
            });
        }
    }
}


import com.google.cloud.bigquery.InsertAllRequest;
import com.mongodb.client.MongoClient;
import com.mongodb.client.MongoClients;
import com.mongodb.client.MongoCollection;
import org.bson.Document;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;

@Service
public class MongoDbService {

    @Value("${mongodb.uri}")
    private String mongoUri;

    @Value("${mongodb.database}")
    private String mongoDatabase;

    @Value("${mongodb.collection}")
    private String mongoCollection;

    public MongoCollection<Document> getCollection() {
        MongoClient mongoClient = MongoClients.create(mongoUri);
        return mongoClient.getDatabase(mongoDatabase).getCollection(mongoCollection);
    }

    public List<InsertAllRequest.RowToInsert> getRowsFromCollection(MongoCollection<Document> collection) {
        List<InsertAllRequest.RowToInsert> rows = new ArrayList<>();
        for (Document document : collection.find()) {
            InsertAllRequest.RowToInsert row = InsertAllRequest.RowToInsert.of(document);
            rows.add(row);
        }
        return rows;
    }
}


import com.google.cloud.bigquery.*;
import com.mongodb.client.MongoCollection;
import org.bson.Document;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

@Service
public class BigQueryService {

    @Value("${bigquery.projectId}")
    private String bigQueryProjectId;

    @Value("${bigquery.datasetId}")
    private String bigQueryDatasetId;

    @Value("${bigquery.tableId}")
    private String bigQueryTableId;

    public BigQuery getBigQueryInstance() {
        return BigQueryOptions.getDefaultInstance().getService();
    }

    public TableId getTableId() {
        return TableId.of(bigQueryProjectId, bigQueryDatasetId, bigQueryTableId);
    }

    public void createTableIfNotExists(MongoCollection<Document> collection, TableId tableId) {
        BigQuery bigquery = getBigQueryInstance();
        Table table = bigquery.getTable(tableId);

        if (table == null) {
            // Infer schema from the first document
            Document firstDocument = collection.find().first();
            Schema schema = SchemaInferenceUtil.inferSchemaFromDocument(firstDocument);

            // Create table with inferred schema and partitioning
            TableDefinition tableDefinition = StandardTableDefinition.newBuilder()
                    .setSchema(schema)
                    .setTimePartitioning(TimePartitioning.newBuilder(TimePartitioning.Type.DAY)
                            .setField("rolldate")
                            .build())
                    .build();
            TableInfo tableInfo = TableInfo.newBuilder(tableId, tableDefinition).build();
            bigquery.create(tableInfo);
        }
    }
}


import com.google.cloud.bigquery.Field;
import com.google.cloud.bigquery.Schema;
import com.google.cloud.bigquery.StandardSQLTypeName;
import org.bson.Document;

import java.util.ArrayList;
import java.util.List;

public class SchemaInferenceUtil {

    public static Schema inferSchemaFromDocument(Document document) {
        List<Field> fields = new ArrayList<>();
        document.forEach((k, v) -> {
            Field.Builder fieldBuilder = Field.newBuilder(k, inferFieldTypeFromValue(v));
            if (k.equals("rolldate")) {
                fieldBuilder.setMode(Field.Mode.NULLABLE);
            }
            fields.add(fieldBuilder.build());
        });
        return Schema.of(fields);
    }

    private static StandardSQLTypeName inferFieldTypeFromValue(Object value) {
        if (value instanceof Integer) {
            return StandardSQLTypeName.INT64;
        } else if (value instanceof Long) {
            return StandardSQLTypeName.INT64;
        } else if (value instanceof Float) {
            return StandardSQLTypeName.FLOAT64;
        } else if (value instanceof Double) {
            return StandardSQLTypeName.FLOAT64;
        } else if (value instanceof Boolean) {
            return StandardSQLTypeName.BOOL;
        } else if (value instanceof String) {
            return StandardSQLTypeName.STRING;
        } else {
            return StandardSQLTypeName.STRING;
        }
    }
}

import io.swagger.v3.oas.models.OpenAPI;
import io.swagger.v3.oas.models.servers.Server;
import org.springdoc.core.GroupedOpenApi;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.reactive.function.client.WebClient;

import java.util.List;

@Configuration
public class SwaggerAggregatorConfig {

    @Value("${query-service.url}")
    private String queryServiceUrl;

    @Value("${scneario-service.url}")
    private String scenarioServiceUrl;

    @Autowired
    private WebClient.Builder webClientBuilder;

    @Bean
    public GroupedOpenApi queryServiceOpenApi() {
        OpenAPI openAPI = fetchOpenApiFromService(queryServiceUrl + "/api-docs");
        return GroupedOpenApi.builder()
                .group("query Service")
                .pathsToMatch("/**")
                .openApiCustomiser(api -> api.servers(List.of(new Server().url(queryServiceUrl))))
                .setGroupOpenApi(openAPI)
                .build();
    }

    @Bean
    public GroupedOpenApi scenarioServiceOpenApi() {
        OpenAPI openAPI = fetchOpenApiFromService(scenarioServiceUrl + "/api-docs");
        return GroupedOpenApi.builder()
                .group("scenario Service")
                .pathsToMatch("/**")
                .openApiCustomiser(api -> api.servers(List.of(new Server().url(scenarioServiceUrl))))
                .setGroupOpenApi(openAPI)
                .build();
    }

    private OpenAPI fetchOpenApiFromService(String apiUrl) {
        return webClientBuilder.build()
                .get()
                .uri(apiUrl)
                .retrieve()
                .bodyToMono(OpenAPI.class)
                .block();
    }
}

// MongoToBigQueryTransfer.java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

@Component
public class MongoToBigQueryTransfer {

    @Autowired
    private MongodbService mongodbService;

    @Autowired
    private BigqueryService bigqueryService;

    @Value("${bigquery.partition.column}")
    private String bqPartitionColumn;

    public void transferData(String dbName, String collectionName, String rollDate, String bqProjectId, String bqDatasetId, String bqTableId) {
        List<Document> documents = mongodbService.getDocumentsByRollDate(dbName, collectionName, rollDate);
        TableSchema schema = SchemaInterferenceUtil.inferSchema(documents);
        bigqueryService.createTable(bqProjectId, bqDatasetId, bqTableId, schema, bqPartitionColumn);
        bigqueryService.insertData(bqProjectId, bqDatasetId, bqTableId, documents);
    }
}

// MongodbService.java
import com.mongodb.client.MongoClient;
import com.mongodb.client.MongoClients;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
import com.mongodb.client.model.Filters;
import org.bson.Document;
import org.bson.conversions.Bson;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import java.util.ArrayList;
import java.util.List;

@Service
public class MongodbService {

    @Value("${mongodb.uri}")
    private String connectionString;

    public List<Document> getDocumentsByRollDate(String dbName, String collectionName, String rollDate) {
        MongoClient mongoClient = MongoClients.create(connectionString);
        MongoDatabase database = mongoClient.getDatabase(dbName);
        MongoCollection<Document> collection = database.getCollection(collectionName);
        Bson filter = Filters.eq("rollDate", rollDate);
        List<Document> documents = new ArrayList<>();
        collection.find(filter).into(documents);
        return documents;
    }
}

// BigqueryService.java
import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryOptions;
import com.google.cloud.bigquery.Field;
import com.google.cloud.bigquery.Schema;
import com.google.cloud.bigquery.StandardTableDefinition;
import com.google.cloud.bigquery.Table;
import com.google.cloud.bigquery.TableDefinition;
import com.google.cloud.bigquery.TableId;
import com.google.cloud.bigquery.TableInfo;
import com.google.cloud.bigquery.TimePartitioning;
import com.google.cloud.bigquery.TimePartitioning.Type;
import org.springframework.stereotype.Service;

@Service
public class BigqueryService {

    public void createTable(String projectId, String datasetId, String tableId, TableSchema schema, String partitionColumn) {
        BigQuery bigquery = BigQueryOptions.getDefaultInstance().getService();
        TableId tableIdentity = TableId.of(projectId, datasetId, tableId);
        Table table = bigquery.getTable(tableIdentity);
        if (table != null) {
            return;
        }
        TableDefinition tableDefinition = StandardTableDefinition.newBuilder()
                .setSchema(schema)
                .setTimePartitioning(TimePartitioning.of(Type.DAY, Field.of(partitionColumn)))
                .build();
        TableInfo tableInfo = TableInfo.newBuilder(tableIdentity, tableDefinition).build();
        bigquery.create(tableInfo);
    }

    // ...existing code for insertData method...
}
import java.util.stream.Collectors;
import java.util.stream.StreamSupport;

@Service
public class MongoToBigQueryTransferService {

    private static final int BATCH_SIZE = 500; // Adjust this value according to your needs

    // ...

    public void transferData() {
        MongoCollection<Document> collection = mongoDbService.getCollection();
        TableId tableId = bigQueryService.getTableId();
        BigQuery bigquery = bigQueryService.getBigQueryInstance();

        // Ensure table exists with the correct schema and partitioning
        bigQueryService.createTableIfNotExists(collection, tableId);

        // Transfer data
        List<InsertAllRequest.RowToInsert> rows = mongoDbService.getRowsFromCollection(collection);

        // Split rows into smaller batches
        for (List<InsertAllRequest.RowToInsert> batch : splitList(rows, BATCH_SIZE)) {
            InsertAllRequest insertRequest = InsertAllRequest.newBuilder(tableId).setRows(batch).build();
            InsertAllResponse insertResponse = bigquery.insertAll(insertRequest);

            if (insertResponse.hasErrors()) {
                System.err.println("Errors occurred while inserting rows:");
                insertResponse.getInsertErrors().forEach((index, errors) -> {
                    System.err.println("Row index: " + index);
                    errors.forEach(error -> System.err.println("Error: " + error.getMessage()));
                });
            }
        }
    }

    private <T> List<List<T>> splitList(List<T> list, int chunkSize) {
        return StreamSupport.stream(Spliterators.spliteratorUnknownSize(list.iterator(), Spliterator.ORDERED), false)
                .collect(Collectors.groupingBy(item -> list.indexOf(item) / chunkSize))
                .values()
                .stream()
                .map(groupedItems -> new ArrayList<>(groupedItems))
                .collect(Collectors.toList());
    }
}


spring:
  cloud:
    gateway:
      routes:
        - id: query-service
          uri: lb://query-service
          predicates:
            - Path=/query-service/**
          filters:
            - StripPrefix=1
        - id: hpl-service-v4
          uri: http://gcp.hpl.com
          predicates:
            - Path=/query-service/api/v4/**
          filters:
            - RewritePath=/query-service/api/v4/(?<path>.*), /$\{path}
