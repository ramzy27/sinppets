

import org.apache.avro.file.DataFileStream;
import org.apache.avro.generic.GenericDatumReader;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DatumReader;
import org.apache.avro.io.DecoderFactory;
import org.springframework.core.io.buffer.DataBuffer;
import org.springframework.core.io.buffer.DefaultDataBufferFactory;

// ...

int bufferSize = 4096; // Adjust the buffer size as needed
ByteBuffer buffer = ByteBuffer.allocate(bufferSize);
long totalBytesRead = 0; // Keep track of the total bytes read

try {
    // Instantiate Avro objects for reading records
    DatumReader<GenericRecord> datumReader = new GenericDatumReader<>();
    BinaryDecoder decoder = null;

    try (ReadChannel reader = blob.reader()) {
        InputStream inputStream = Channels.newInputStream(reader);
        
        // We need to read the Avro header first (this code assumes the schema is embedded)
        DataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inputStream, datumReader);
        Schema schema = dataFileStream.getSchema();
        datumReader.setSchema(schema);
        
        // Now read the blocks
        while (dataFileStream.hasNext()) {
            GenericRecord record = dataFileStream.next();
            
            // Serialize the record to a byte array or directly to a DataBuffer
            // This part depends on how you want to serialize your records
            byte[] serializedRecord = serializeRecord(record, schema); // Implement this method
            
            // Wrap the byte array in a DataBuffer
            DataBuffer dataBuffer = new DefaultDataBufferFactory().wrap(serializedRecord);
            
            // Emit the DataBuffer to the Flux sink
            sink.next(dataBuffer);
        }
    }
    // Complete the Flux when done
    sink.complete();
} catch (Exception e) {
    sink.error(e);
}


import base64
import json

# Replace this with your JSON array copied from Postman
json_array = [
    {"nativeBuffer": "base64encodedstring1=="},
    {"nativeBuffer": "base64encodedstring2=="}
]

# Decode each base64 string and concatenate the binary data
binary_data = b''.join(base64.b64decode(chunk['nativeBuffer']) for chunk in json_array)

# Write the binary data to an Avro file
with open('output.avro', 'wb') as avro_file:
    avro_file.write(binary_data)

# Now you can use Avro tools to validate 'output.avro'
@GetMapping("/data")
public Mono<ResponseEntity<Flux<DataBuffer>>> getData() {
    Flux<DataBuffer> dataFlux = createDataBufferFlux(); // Replace with your actual data fetching logic

    return Mono.just(ResponseEntity.ok()
            .contentType(MediaType.APPLICATION_OCTET_STREAM) // or MediaType.APPLICATION_JSON if applicable
            .body(dataFlux))
        .onErrorResume(e -> Mono.just(
            ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
            .build()));
}

int bufferSize = 4096; // Adjust the buffer size as needed
ByteBuffer buffer = ByteBuffer.allocate(bufferSize);
int bytesRead;
long totalBytesRead = 0; // Keep track of the total bytes read

try {
    while ((bytesRead = blob.reader().read(buffer)) != -1) {
        totalBytesRead += bytesRead; // Update the total bytes read
        System.out.println("Bytes read this iteration: " + bytesRead);
        System.out.println("Total bytes read: " + totalBytesRead);

        // Ensure that you do not read past the end of the file
        if (totalBytesRead >= 312 * 1024) { // 312 KB in bytes
            System.out.println("Read past the expected file size, breaking out of loop.");
            break;
        }

        byte[] byteArray = new byte[bytesRead];
        buffer.flip();
        buffer.get(byteArray);
        DataBuffer dataBuffer = bufferFactory.wrap(byteArray);

        // Emit the DataBuffer to the Flux sink
        sink.next(dataBuffer);

        // Clear the buffer for the next read
        buffer.clear();
    }

    // Complete the Flux when done
    sink.complete();
    System.out.println("Reading process completed successfully.");
} catch (Exception e) {
    System.err.println("An error occurred during file reading: " + e.getMessage());
    sink.error(e);
}




}
@Service
public class DataExportService {
    @Autowired
    private BigQuery bigQuery; // Autowire the BigQuery client

    @Autowired
    private Storage storage; // Autowire the GCS storage client

    public String exportDataToGcs() throws IOException, InterruptedException {
        // Define your SQL query
        String sqlQuery = "SELECT * FROM your_dataset.your_table";

        // Set the GCS bucket and file name
        String bucketName = "your-gcs-bucket";
        String fileName = "exported-data.avro";

        // Create a JobConfiguration for the query
        JobConfigurationQuery queryConfig =
                JobConfigurationQuery.newBuilder(sqlQuery)
                        .setDestinationTable(TableId.of("your-project-id", "your_dataset", "temp_table"))
                        .build();

        // Create a BigQuery job
        Job job = bigQuery.create(JobInfo.of(queryConfig));

        // Wait for the job to complete
        job = job.waitFor();

        if (job.getStatus().getError() != null) {
            throw new RuntimeException("BigQuery job failed with error: " + job.getStatus().getError());
        }

        // Export the result to GCS
        TableId tableId = job.getConfiguration().getQuery().getDestinationTable();
        String destinationUri = "gs://" + bucketName + "/" + fileName;

        ExtractJobConfiguration extractConfig =
                ExtractJobConfiguration.newBuilder(tableId, destinationUri)
                        .setFormatOptions(FormatOptions.avro())
                        .build();

        Job extractJob = bigQuery.create(JobInfo.of(extractConfig));
        extractJob = extractJob.waitFor();

        if (extractJob.getStatus().getError() != null) {
            throw new RuntimeException("Export job failed with error: " + extractJob.getStatus().getError());
        }

        return fileName; // Return the ID (file name) for later retrieval
    }
}
```


@RestController
@RequestMapping("/api/data-export")
public class DataExportController {
    @Autowired
    private DataExportService dataExportService;

    @PostMapping("/export")
    public ResponseEntity<String> exportDataToGcs() {
        try {
            String fileId = dataExportService.exportDataToGcs();
            return ResponseEntity.ok(fileId);
        } catch (IOException | InterruptedException e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body("Export failed: " + e.getMessage());
        }
    }
}
```

3. **Configure BigQuery and GCS:** Ensure that you have configured your Spring application to use the Google Cloud BigQuery and GCS libraries. You will need to provide credentials and project configuration to access these services.

4. **Configure Routing:** Configure the routing for your Spring application to map the controller's endpoints to URLs.

Now, when you make a POST request to `/api/data-export/export`, it will trigger the export process, and the endpoint will return the ID (file name) that can be used to fetch the exported Avro file from the GCS bucket later. Make sure to replace `"your_dataset.your_table"` and other placeholders with your actual dataset, table, GCS bucket, and project information.

Please ensure that you have the necessary dependencies and configurations for Google Cloud libraries, BigQuery, and GCS in your Spring application.
